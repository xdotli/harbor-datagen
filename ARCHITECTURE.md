# Harbor-Datagen Architecture

This document provides a comprehensive technical overview of the harbor-datagen system architecture, designed for engineers working on agent-driven data generation, evaluation infrastructure, or benchmark development.

## Table of Contents

- [Overview](#overview)
- [Harbor Task Format Specification](#harbor-task-format-specification)
- [Agent-Driven Data Generation Workflow](#agent-driven-data-generation-workflow)
- [Task Validation Pipeline](#task-validation-pipeline)
- [Harbor Evaluation Pipeline](#harbor-evaluation-pipeline)
- [Example Task Deep-Dive: auth_token_race_condition](#example-task-deep-dive-auth_token_race_condition)
- [Reusable Components for New Datasets](#reusable-components-for-new-datasets)
- [Comparison with Other Frameworks](#comparison-with-other-frameworks)
- [Best Practices & Guidelines](#best-practices--guidelines)

---

## Overview

Harbor-datagen is a **proof-of-concept system for using AI coding agents to generate synthetic evaluation tasks** in Harbor format. The project demonstrates how agents can scale dataset creation for both evaluation and training of AI systems.

**Key Innovation**: Instead of manually crafting bug-fixing tasks, agents generate realistic software engineering challenges by:
1. Conceiving bug scenarios (race conditions, async issues, database bugs)
2. Implementing buggy environments with Docker
3. Writing comprehensive test suites
4. Creating reference solutions
5. Self-validating by attempting to solve their own tasks

**Current Status**: Contains 3 validated tasks (2 production-ready) generated by Claude Sonnet 4.5 agents, demonstrating 91-100% success rates when evaluated.

**Architecture**: Built on the Harbor framework, harbor-datagen produces tasks compatible with Harbor's orchestration system, enabling evaluation of diverse coding agents (Claude Code, OpenHands, Codex CLI, etc.) across thousands of parallel environments.

---

## Harbor Task Format Specification

Each task follows a standardized directory structure that enables reproducible, isolated evaluation:

```
<task-name>/
â”œâ”€â”€ task.toml              # Metadata and resource configuration
â”œâ”€â”€ instruction.md         # Natural language problem description (what agents see)
â”œâ”€â”€ environment/           # Buggy application environment
â”‚   â”œâ”€â”€ Dockerfile        # Container build instructions
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies (pinned versions)
â”‚   â”œâ”€â”€ start.sh          # Service initialization script
â”‚   â”œâ”€â”€ start_*.sh        # Individual service startup scripts
â”‚   â””â”€â”€ app/              # Application code with intentional bugs
â”œâ”€â”€ solution/             # Reference implementation
â”‚   â”œâ”€â”€ solve.sh         # Script that applies the fix
â”‚   â””â”€â”€ reference/       # Working code files
â”œâ”€â”€ tests/                # Verification and grading
â”‚   â”œâ”€â”€ test.sh          # Main test runner (entry point)
â”‚   â””â”€â”€ test_*.py        # Test cases (pytest)
â””â”€â”€ example_runs/         # Historical agent execution data
    â””â”€â”€ <run-id>/
        â”œâ”€â”€ agent/       # Agent trajectory and commands
        â”œâ”€â”€ verifier/    # Test output and rewards
        â””â”€â”€ result.json  # Execution metadata
```

### Core Files Specification

#### 1. `task.toml` - Task Configuration

Defines metadata and resource constraints:

```toml
version = "1.0"

[metadata]
author_name = "Xiangyi Li"
author_email = "xiangyi@benchflow.ai"
difficulty = "hard"                    # easy, medium, hard
category = "backend-engineering"       # Task domain
tags = ["python", "fastapi", "redis", "race-condition", "concurrency"]

[verifier]
timeout_sec = 180.0                    # Time to run tests

[agent]
timeout_sec = 600.0                    # Time for agent to solve

[environment]
build_timeout_sec = 600.0              # Docker build time
cpus = 2                               # CPU cores
memory_mb = 4096                       # RAM allocation (4GB)
storage_mb = 20480                     # Disk space (20GB)
```

**Key Configuration Decisions**:
- **Agent timeout (600s)**: Standard for most tasks; complex debugging may need 900s
- **Verifier timeout (180s)**: Must accommodate service startup + test execution
- **Build timeout (600-900s)**: Depends on dependency installation complexity
- **Resource limits**: Standard 2 CPU / 4GB RAM; database-heavy tasks may need 8GB

#### 2. `instruction.md` - Problem Description

Natural language description from a user's perspective:

```markdown
Our auth gateway is having issues under load - users are getting randomly 
logged out and seeing "invalid refresh token" errors when multiple requests 
hit at the same time. The `/auth/refresh` endpoint works fine with sequential 
requests but fails intermittently when the same refresh token gets processed 
by multiple gateway instances concurrently. Pretty sure there's a race 
condition in the token refresh logic where the read-validate-delete-create 
operations aren't atomic.
```

**Design Principles**:
- **User voice**: Describe symptoms, not root causes
- **Context**: Explain when/how the issue manifests
- **Realistic**: Language an engineer would actually use
- **Minimal technical jargon**: Avoid revealing the exact solution

#### 3. `environment/Dockerfile` - Isolated Execution Environment

Multi-stage Docker setup with all dependencies:

```dockerfile
FROM python:3.11-slim

# Install system dependencies (Redis, PostgreSQL, etc.)
RUN apt-get update && apt-get install -y redis-server && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

# Install Python dependencies (pinned versions)
COPY requirements.txt /workspace/
RUN pip install --no-cache-dir --break-system-packages -r requirements.txt

# Copy buggy application code
COPY auth_gateway/ /workspace/auth_gateway/

# Copy service startup scripts
COPY start_redis.sh start_gateway.sh start.sh /workspace/
RUN chmod +x /workspace/*.sh

# Default command starts all services
CMD /workspace/start_redis.sh && /workspace/start_gateway.sh
```

**Critical Requirements**:
- **Reproducibility**: Pin all dependency versions
- **Isolation**: No external network dependencies during evaluation
- **Health checks**: Services must expose readiness endpoints
- **Startup scripts**: Reliable initialization with proper wait logic

#### 4. `tests/test.sh` - Test Orchestration

Entry point for verification:

```bash
#!/bin/bash
set -e

# Start services in background
/workspace/start.sh &
APP_PID=$!

mkdir -p /logs/verifier

# Wait for service readiness (with timeout)
timeout=30
counter=0
until curl -s http://localhost:8000/health > /dev/null 2>&1; do
    sleep 1
    counter=$((counter + 1))
    if [ $counter -ge $timeout ]; then
        echo "Service failed to start"
        echo "0" > /logs/verifier/reward.txt
        exit 1
    fi
done

# Execute pytest test suite
python -m pytest /tests/test_outputs.py -v --tb=short

# Write binary reward (1 = pass, 0 = fail)
if [ $? -eq 0 ]; then
    echo "1" > /logs/verifier/reward.txt
else
    echo "0" > /logs/verifier/reward.txt
fi
```

**Design Patterns**:
- **Service readiness**: Always wait for health checks before testing
- **Timeout handling**: Fail gracefully if services don't start
- **Reward format**: Binary 0/1 written to `/logs/verifier/reward.txt`
- **Exit codes**: Non-zero exit on any failure

#### 5. `tests/test_outputs.py` - Test Implementation

Comprehensive pytest test suite:

```python
import pytest
import pytest_asyncio
import httpx

@pytest.mark.asyncio
async def test_concurrent_refresh_same_token(http_client, redis_client):
    """
    Test concurrent refresh token requests to validate atomicity.
    Issues 10 concurrent requests with the same token.
    Exactly ONE should succeed (atomic operations).
    """
    tokens = await create_test_user(http_client, "concurrent_user")
    refresh_token = tokens["refresh_token"]
    
    # Send 10 concurrent requests
    tasks = [refresh_token_request(http_client, refresh_token) for _ in range(10)]
    responses = await asyncio.gather(*tasks)
    
    success_count = sum(1 for r in responses if r.status_code == 200)
    failure_count = sum(1 for r in responses if r.status_code == 401)
    
    # With atomic operations, exactly ONE succeeds
    assert success_count == 1, f"Expected exactly 1 success, got {success_count}"
    assert failure_count == 9, f"Expected 9 failures, got {failure_count}"
```

**Test Design Principles**:
- **Comprehensive coverage**: Success cases, edge cases, failure modes
- **Concurrency testing**: Stress concurrent operations to expose race conditions
- **Deterministic**: Same code should produce same results
- **Clear assertions**: Explicit expected values with helpful error messages
- **Performance**: Complete within verifier timeout (typically 180s)

#### 6. `solution/solve.sh` - Reference Solution

Script that applies the fix:

```bash
#!/bin/bash
# Solution: Restore atomic Redis operations

# Copy working implementation
cp /solution/reference/auth_gateway/token_service.py /workspace/auth_gateway/token_service.py

# Restart service to load fixed code
pkill -f uvicorn
sleep 2
cd /workspace && python -m uvicorn auth_gateway.main:app --host 0.0.0.0 --port 8000 --workers 3 &

sleep 3
echo "Fixed: Restored atomic token refresh using Redis Lua scripts"
```

**Solution Characteristics**:
- **Idempotent**: Can be run multiple times safely
- **Self-contained**: No external dependencies
- **Verified**: Must pass all tests when applied
- **Documented**: Comment explains the fix approach

---

## Agent-Driven Data Generation Workflow

Harbor-datagen leverages AI agents (Claude Sonnet 4.5) to generate tasks through an iterative, multi-phase process:

### Phase 1: Task Conception

**Input**: High-level constraints (difficulty, category, technology stack)

**Agent Activity**:
- Proposes realistic bug scenarios based on common software engineering challenges
- Selects appropriate technology stack (FastAPI, Redis, PostgreSQL, etc.)
- Defines bug characteristics (race condition, async issue, database transaction bug)
- Estimates task difficulty and resource requirements

**Output**: Task proposal with problem description and technical approach

### Phase 2: Environment Creation

**Agent Activity**:
- Writes `Dockerfile` with appropriate base image and dependencies
- Creates `requirements.txt` with pinned package versions
- Implements application code with intentional bugs
- Writes service startup scripts (`start.sh`, `start_redis.sh`, etc.)
- Configures multi-service environments (app + database + cache)

**Critical Considerations**:
- **Bug realism**: Bugs must be subtle, not obvious typos
- **Reproducibility**: Environment must build deterministically
- **Isolation**: No external network dependencies
- **Health checks**: Services must expose readiness endpoints

**Output**: Complete `environment/` directory with buggy application

### Phase 3: Test Development

**Agent Activity**:
- Designs comprehensive test strategy (unit, integration, concurrency, stress)
- Implements pytest test suite in `tests/test_outputs.py`
- Writes `test.sh` orchestration script
- Validates test coverage (success cases, edge cases, failure modes)
- Ensures tests fail on buggy code, pass on fixed code

**Test Coverage Strategy** (example for `auth_token_race_condition`):
1. **Concurrency test (40% weight)**: 10 concurrent requests, exactly 1 succeeds
2. **Correctness test (25% weight)**: Sequential refresh rotation works
3. **Stress test (35% weight)**: 100 concurrent requests across 20 users

**Output**: Complete `tests/` directory with executable test suite

### Phase 4: Solution Implementation

**Agent Activity**:
- Implements fix for the intentional bugs
- Creates `solution/reference/` with working code
- Writes `solution/solve.sh` to apply the fix
- Tests solution against test suite
- Verifies all tests pass with solution

**Output**: Complete `solution/` directory with verified fix

### Phase 5: Self-Validation

**Agent Activity**:
- Attempts to solve the task end-to-end (as a solver agent would)
- Validates that the task is solvable within timeout constraints
- Identifies ambiguities or issues in `instruction.md`
- Adjusts difficulty calibration if needed

**Validation Checks**:
- Docker environment builds successfully
- All services start correctly
- Tests are discoverable and executable
- Tests fail without solution, pass with solution
- Agent can solve the task within timeout

**Output**: Validated task with `example_runs/` demonstrating solvability

### Phase 6: Human Review and Iteration

**Human Activity**:
- Reviews task realism and difficulty calibration
- Validates test coverage and quality
- Checks for security issues or inappropriate content
- Approves or requests revisions

**Status Markers** (in `README.md`):
- **âœ… READY FOR PRODUCTION**: Fully validated, agent can solve it
- **âš ï¸ VALIDATED**: Infrastructure works, agent achieves partial success  
- **ğŸš§ IN DEVELOPMENT**: Task is being built and tested
- **âŒ DEPRECATED**: Task is no longer maintained

**Output**: Production-ready task added to dataset

### Data Generation Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT-DRIVEN DATA GENERATION                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1: Task Conception
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Agent (Claude 4.5)   â”‚
â”‚  - Propose bug scenario  â”‚
â”‚  - Select tech stack     â”‚
â”‚  - Define difficulty     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Task Proposal Document  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Phase 2: Environment Creation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Agent writes:        â”‚
â”‚  â”œâ”€ Dockerfile          â”‚
â”‚  â”œâ”€ requirements.txt    â”‚
â”‚  â”œâ”€ Buggy app code      â”‚
â”‚  â””â”€ Startup scripts     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  environment/ directory  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Phase 3: Test Development
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Agent writes:        â”‚
â”‚  â”œâ”€ test.sh (runner)    â”‚
â”‚  â”œâ”€ test_*.py (pytest)  â”‚
â”‚  â””â”€ Test strategy       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  tests/ directory        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Phase 4: Solution Implementation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Agent writes:        â”‚
â”‚  â”œâ”€ Fixed code          â”‚
â”‚  â””â”€ solve.sh script     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  solution/ directory     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Phase 5: Self-Validation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Agent attempts solve â”‚
â”‚  - Build environment    â”‚
â”‚  - Run as solver        â”‚
â”‚  - Validate solvability â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  example_runs/ data      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Phase 6: Human Review
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Human reviewer:         â”‚
â”‚  - Check realism        â”‚
â”‚  - Validate quality     â”‚
â”‚  - Approve/reject       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… Production Task      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Task Validation Pipeline

The validation pipeline ensures tasks are production-ready through automated and manual checks:

### Stage 1: Build Validation

**Objective**: Verify Docker environment builds successfully

**Process**:
```bash
cd <task-name>/environment
docker build -t <task-name>-test .
```

**Success Criteria**:
- Build completes without errors
- All dependencies install correctly
- Image size is reasonable (< 2GB for most tasks)
- Build time within `build_timeout_sec` (typically 600s)

**Common Failures**:
- Missing dependencies in `requirements.txt`
- Network timeout during package installation
- Base image architecture mismatch

### Stage 2: Service Validation

**Objective**: Verify all services start and become ready

**Process**:
```bash
docker run -d <task-name>-test
docker exec <container> /workspace/start.sh
# Wait for health checks
curl http://localhost:8000/health
redis-cli ping
pg_isready -h localhost
```

**Success Criteria**:
- All services start without errors
- Health checks respond within 30 seconds
- Services accept connections on expected ports
- No crashes or segfaults

**Common Failures**:
- Port conflicts
- Missing environment variables
- Database initialization errors
- Insufficient startup wait time

### Stage 3: Test Discovery and Execution

**Objective**: Verify tests are discoverable and executable

**Process**:
```bash
docker exec <container> bash /tests/test.sh
```

**Success Criteria**:
- `test.sh` is executable
- pytest discovers all test functions
- Tests run to completion (no hangs)
- Execution completes within `verifier.timeout_sec`

**Common Failures**:
- Missing pytest markers (`@pytest.mark.asyncio`)
- Import errors in test files
- Test hangs waiting for services
- Timeout exceeded

### Stage 4: Correctness Validation

**Objective**: Verify tests pass with solution, fail without

**Process**:
```bash
# Test buggy code (should fail)
docker run <task-name>-test bash /tests/test.sh
cat /logs/verifier/reward.txt  # Should be "0"

# Apply solution
docker exec <container> bash /solution/solve.sh

# Test fixed code (should pass)
docker exec <container> bash /tests/test.sh
cat /logs/verifier/reward.txt  # Should be "1"
```

**Success Criteria**:
- Tests fail (reward = 0) on buggy code
- Tests pass (reward = 1) on fixed code
- Reward file is written correctly
- Test output is informative

**Common Failures**:
- Tests pass on buggy code (bug is trivial or not tested)
- Tests still fail on fixed code (incomplete solution)
- Reward file not written
- Flaky tests (non-deterministic results)

### Stage 5: Agent Validation

**Objective**: Verify an agent can solve the task end-to-end

**Process**:
```bash
harbor trials start \
  --task-path <task-name> \
  --agent claude-code \
  --model claude-sonnet-4-5
```

**Success Criteria**:
- Agent completes within `agent.timeout_sec` (typically 600s)
- Agent achieves reward >= 0.9 (ideally 1.0)
- Execution is stable (no infrastructure errors)
- Token usage is reasonable (< 2M input tokens)

**Success Rate Targets**:
- **Easy**: Agent should succeed 80%+ of the time
- **Medium**: Agent should succeed 50-70% of the time
- **Hard**: Agent should succeed 30-50% of the time

**Common Failures**:
- Agent times out (task too difficult or ambiguous)
- Agent gets stuck in loops
- Agent misunderstands problem description
- Infrastructure errors (Docker issues, network problems)

### Stage 6: Quality Review

**Objective**: Human review for realism and quality

**Review Checklist**:
- [ ] Problem description is realistic (not contrived)
- [ ] Bug is subtle and interesting (not obvious typo)
- [ ] Tests are comprehensive (cover edge cases)
- [ ] Difficulty calibration is appropriate
- [ ] Code style is professional (no lazy patterns)
- [ ] Security concerns are addressed (no sensitive data leaks)
- [ ] Resource limits are appropriate
- [ ] Documentation is clear and complete

**Output**: Status marker in `README.md`

---

## Harbor Evaluation Pipeline

Harbor orchestrates the complete evaluation lifecycle for coding agents:

### Trial Execution Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HARBOR TRIAL EXECUTION                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Environment Setup
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Harbor Trial Runner                                             â”‚
â”‚  â”œâ”€ Load task from task.toml                                    â”‚
â”‚  â”œâ”€ Create Docker container from environment/Dockerfile         â”‚
â”‚  â”œâ”€ Start container with resource limits (CPU, memory)          â”‚
â”‚  â”œâ”€ Upload task files to container                              â”‚
â”‚  â””â”€ Execute start.sh to initialize services                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (3-5 seconds)
                       â–¼
Step 2: Agent Setup
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent Installation (Claude Code, OpenHands, etc.)              â”‚
â”‚  â”œâ”€ Install agent runtime in container                          â”‚
â”‚  â”œâ”€ Configure agent with model API keys                         â”‚
â”‚  â”œâ”€ Set working directory to /workspace                         â”‚
â”‚  â””â”€ Initialize agent session                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (15-30 seconds)
                       â–¼
Step 3: Agent Execution
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent Problem-Solving Loop                                      â”‚
â”‚  â”œâ”€ Agent reads instruction.md                                  â”‚
â”‚  â”œâ”€ Agent explores codebase                                     â”‚
â”‚  â”œâ”€ Agent diagnoses bug                                         â”‚
â”‚  â”œâ”€ Agent implements fix                                        â”‚
â”‚  â””â”€ Agent validates solution (optional)                         â”‚
â”‚                                                                  â”‚
â”‚  Captured Data:                                                  â”‚
â”‚  â”œâ”€ agent/trajectory.json (action sequence)                     â”‚
â”‚  â”œâ”€ agent/command-*/ (command executions)                       â”‚
â”‚  â””â”€ agent/sessions/ (IDE session data)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (240-600 seconds)
                       â–¼
Step 4: Verification
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Harbor Verifier                                                 â”‚
â”‚  â”œâ”€ Upload tests/ directory to container â†’ /tests               â”‚
â”‚  â”œâ”€ Execute: bash /tests/test.sh | tee test-stdout.txt          â”‚
â”‚  â”œâ”€ Capture stdout/stderr to verifier/test-stdout.txt           â”‚
â”‚  â”œâ”€ Download /logs/verifier/reward.txt                          â”‚
â”‚  â””â”€ Parse reward: "1" or "0" â†’ float                            â”‚
â”‚                                                                  â”‚
â”‚  Output:                                                         â”‚
â”‚  â”œâ”€ verifier/test-stdout.txt                                    â”‚
â”‚  â”œâ”€ verifier/reward.txt                                         â”‚
â”‚  â””â”€ VerifierResult(rewards={'reward': 1.0})                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (30-180 seconds)
                       â–¼
Step 5: Result Collection
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trial Result Generation                                         â”‚
â”‚  â”œâ”€ Aggregate timing data (setup, execution, verification)      â”‚
â”‚  â”œâ”€ Collect agent metrics (tokens, cost, commands)              â”‚
â”‚  â”œâ”€ Package verifier results (reward, test output)              â”‚
â”‚  â”œâ”€ Capture exception info (if any failures)                    â”‚
â”‚  â””â”€ Write result.json                                           â”‚
â”‚                                                                  â”‚
â”‚  Result Structure:                                               â”‚
â”‚  {                                                               â”‚
â”‚    "trial_name": "auth_token_race_condition__abc123",           â”‚
â”‚    "agent_info": {"name": "claude-code", ...},                  â”‚
â”‚    "agent_result": {"n_input_tokens": 1706720, ...},           â”‚
â”‚    "verifier_result": {"rewards": {"reward": 1.0}},            â”‚
â”‚    "timing": {...}                                               â”‚
â”‚  }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
Step 6: Cleanup
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Container Cleanup                                               â”‚
â”‚  â”œâ”€ Stop Docker container                                       â”‚
â”‚  â”œâ”€ Remove container (optional, based on config)                â”‚
â”‚  â””â”€ Upload results to storage (if configured)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components Integration

#### Harbor Trial Runner (`harbor/src/harbor/trial/trial.py`)

**Responsibilities**:
- Orchestrates the complete trial lifecycle
- Manages timeouts for each phase
- Captures and handles exceptions
- Coordinates environment, agent, and verifier

**Key Methods**:
```python
class Trial:
    async def run(self) -> TrialResult:
        # 1. Setup environment
        await self._setup_environment()
        
        # 2. Setup agent
        await self._setup_agent()
        
        # 3. Execute agent
        try:
            await self._execute_agent()
        except AgentTimeoutError:
            # Handle timeout gracefully
            pass
        
        # 4. Run verification
        if not self.config.verifier.disable:
            await self._run_verification()
        
        return self._result
```

#### Harbor Verifier (`harbor/src/harbor/verifier/verifier.py`)

**Responsibilities**:
- Uploads tests to container
- Executes test.sh script
- Captures test output
- Parses reward file

**Verification Process**:
```python
async def verify(self) -> VerifierResult:
    # 1. Upload tests directory
    await self._environment.upload_dir(
        source_dir=self._task.paths.tests_dir,
        target_dir="/tests"
    )
    
    # 2. Execute test script
    result = await self._environment.exec(
        command="bash /tests/test.sh | tee /logs/verifier/test-stdout.txt 2>&1"
    )
    
    # 3. Download verifier output
    await self._environment.download_dir(
        source_dir="/logs/verifier",
        target_dir=self._trial_paths.verifier_dir
    )
    
    # 4. Parse reward
    reward = float(self._trial_paths.reward_text_path.read_text())
    
    return VerifierResult(rewards={"reward": reward})
```

#### Reward Calculation

Harbor supports two reward formats:

**1. Text Format** (`/logs/verifier/reward.txt`):
```
1
```
Parsed as: `{"reward": 1.0}`

**2. JSON Format** (`/logs/verifier/reward.json`):
```json
{
  "reward": 1.0,
  "test_passed": 3,
  "test_failed": 0,
  "test_total": 3
}
```

**Reward Semantics**:
- **1.0**: Perfect solution, all tests passed
- **0.0**: Solution failed, tests did not pass
- **0.0-1.0**: Partial credit (e.g., 0.91 for 10/11 tests passing)

Harbor-datagen tasks primarily use **binary rewards (0 or 1)** for simplicity.

### Integration with Harbor Framework

Harbor-datagen tasks are **first-class citizens** in the Harbor ecosystem:

#### Task Registration

Tasks can be registered in Harbor's dataset registry:

```bash
# Run evaluation on harbor-datagen tasks
harbor run \
  --dataset harbor-datagen \
  --agent claude-code \
  --model anthropic/claude-sonnet-4-5 \
  --n-concurrent 4
```

#### Task Discovery

Harbor discovers tasks via:
- **Local path**: `--task-path /path/to/task-dir`
- **Dataset registry**: `--dataset harbor-datagen`
- **Git repository**: `--git-url https://github.com/...`

#### Environment Providers

Harbor supports multiple execution backends:
- **Docker** (local): Fast iteration, local development
- **Daytona**: Cloud-based, high parallelism (100+ concurrent)
- **Modal**: Serverless, auto-scaling
- **E2B**: Sandboxed environments

Example with Daytona:
```bash
harbor run \
  --dataset harbor-datagen \
  --agent claude-code \
  --model anthropic/claude-sonnet-4-5 \
  --env daytona \
  --n-concurrent 100
```

---

## Example Task Deep-Dive: auth_token_race_condition

Let's examine a complete, production-ready task to understand the architecture in practice.

### Task Overview

**Difficulty**: Hard  
**Category**: Backend Engineering  
**Tags**: python, fastapi, redis, race-condition, concurrency, distributed-systems, jwt, authentication  
**Agent Success Rate**: 100% (1/1 runs with Claude Code + Sonnet 4.5)

**Bug Description**: Non-atomic Redis operations in JWT token refresh endpoint cause race conditions when multiple gateway instances process the same refresh token concurrently. Users experience random logouts under load.

### Problem Statement (`instruction.md`)

```
Our auth gateway is having issues under load - users are getting randomly 
logged out and seeing "invalid refresh token" errors when multiple requests 
hit at the same time. The `/auth/refresh` endpoint works fine with sequential 
requests but fails intermittently when the same refresh token gets processed 
by multiple gateway instances concurrently. Pretty sure there's a race 
condition in the token refresh logic where the read-validate-delete-create 
operations aren't atomic.
```

**Analysis**:
- **User perspective**: Describes symptoms, not root cause
- **Contextual clues**: "under load", "multiple requests", "same token"
- **Hint at solution**: "operations aren't atomic"
- **Realistic**: Language a backend engineer would use

### Environment Setup

**Services**:
- **FastAPI gateway**: Multi-worker JWT authentication service (port 8000)
- **Redis**: Token storage backend (port 6379)

**Docker Setup** (`environment/Dockerfile`):
```dockerfile
FROM python:3.11-slim

# Install Redis server
RUN apt-get update && apt-get install -y redis-server

WORKDIR /workspace

# Install dependencies
COPY requirements.txt /workspace/
RUN pip install --no-cache-dir -r requirements.txt

# Copy buggy application code
COPY auth_gateway/ /workspace/auth_gateway/

# Copy startup scripts
COPY start_redis.sh start_gateway.sh start.sh /workspace/
RUN chmod +x /workspace/*.sh

CMD /workspace/start.sh
```

**Service Startup** (`environment/start.sh`):
```bash
#!/bin/bash
set -e

# Start Redis
/workspace/start_redis.sh &

# Wait for Redis to be ready
until redis-cli ping > /dev/null 2>&1; do
    sleep 1
done

# Start FastAPI gateway with 3 workers (simulates multiple instances)
/workspace/start_gateway.sh &

# Wait for gateway to be ready
until curl -s http://localhost:8000/health > /dev/null 2>&1; do
    sleep 1
done

echo "All services ready"
```

### The Bug

**File**: `environment/auth_gateway/token_service.py`  
**Lines**: 84-136

**Buggy Code** (simplified):
```python
async def refresh_token(self, refresh_token: str) -> Dict[str, str]:
    """Non-atomic refresh token rotation (BUGGY)"""
    
    # 1. Read token from Redis
    token_data_str = await self.redis.get(f"refresh_token:{refresh_token}")
    if not token_data_str:
        raise HTTPException(status_code=401, detail="Invalid refresh token")
    
    token_data = json.loads(token_data_str)
    
    # 2. Validate token (decode JWT, check expiry)
    self._validate_refresh_token(token_data)
    
    # âš ï¸ RACE CONDITION HERE âš ï¸
    # Another request can process the same token between steps 2 and 3
    
    # 3. Delete old token
    await self.redis.delete(f"refresh_token:{refresh_token}")
    
    # 4. Create new tokens
    new_access_token = self._generate_access_token(token_data["user_id"])
    new_refresh_token = self._generate_refresh_token()
    
    # 5. Store new refresh token
    await self.redis.set(
        f"refresh_token:{new_refresh_token}",
        json.dumps({"user_id": token_data["user_id"], ...})
    )
    
    return {
        "access_token": new_access_token,
        "refresh_token": new_refresh_token
    }
```

**Race Condition Scenario**:
1. Request A reads refresh token (exists) âœ“
2. Request B reads refresh token (exists) âœ“
3. Request A validates token âœ“
4. Request B validates token âœ“
5. Request A deletes token
6. Request A creates new token X
7. Request B deletes token **â† ERROR: Deletes Request A's new token X!**
8. Request B creates new token Y
9. **Result**: User holds token X, but token X was deleted. Next request fails!

### Test Strategy

**File**: `tests/test_outputs.py`

**Test 1: Concurrent Refresh with Same Token** (Weight: 40%)
```python
@pytest.mark.asyncio
async def test_concurrent_refresh_same_token(http_client, redis_client):
    """
    Send 10 concurrent refresh requests with the same token.
    With atomic operations, exactly ONE should succeed.
    """
    tokens = await create_test_user(http_client, "user1")
    refresh_token = tokens["refresh_token"]
    
    # 10 concurrent requests
    tasks = [refresh_token_request(http_client, refresh_token) for _ in range(10)]
    responses = await asyncio.gather(*tasks)
    
    success_count = sum(1 for r in responses if r.status_code == 200)
    failure_count = sum(1 for r in responses if r.status_code == 401)
    
    # Atomic operations: exactly 1 succeeds, 9 fail
    assert success_count == 1
    assert failure_count == 9
```

**Test 2: Sequential Refresh Rotation** (Weight: 25%)
```python
@pytest.mark.asyncio
async def test_sequential_refresh_rotation(http_client, redis_client):
    """
    Perform 5 sequential refresh operations.
    Each should succeed, and old tokens should be invalidated.
    """
    tokens = await create_test_user(http_client, "user2")
    
    for i in range(5):
        current_refresh = tokens["refresh_token"]
        
        # Refresh should succeed
        response = await refresh_token_request(http_client, current_refresh)
        assert response.status_code == 200
        
        new_tokens = response.json()
        
        # Old token should be invalid
        old_response = await refresh_token_request(http_client, current_refresh)
        assert old_response.status_code == 401
        
        tokens = new_tokens
```

**Test 3: Multi-User Concurrent Stress** (Weight: 35%)
```python
@pytest.mark.asyncio
async def test_multi_user_concurrent_stress(http_client, redis_client):
    """
    Create 20 users, send 100 concurrent requests (5 per user).
    Each user should have exactly 1 success and 4 failures.
    """
    users = []
    for i in range(20):
        tokens = await create_test_user(http_client, f"user{i}")
        users.append({"username": f"user{i}", "tokens": tokens})
    
    # 100 concurrent requests (5 per user)
    tasks = []
    for user in users:
        for _ in range(5):
            tasks.append(refresh_token_request(http_client, user["tokens"]["refresh_token"]))
    
    responses = await asyncio.gather(*tasks)
    
    total_success = sum(1 for r in responses if r.status_code == 200)
    total_failure = sum(1 for r in responses if r.status_code == 401)
    
    assert total_success == 20  # 1 per user
    assert total_failure == 80  # 4 per user
```

**Test Coverage Analysis**:
- **Concurrency**: Validates atomic operations under concurrent load
- **Correctness**: Ensures normal token rotation works
- **Stress**: Tests system under realistic load (multiple users)
- **State integrity**: Verifies Redis state is consistent

### Solution Approach

**File**: `solution/reference/auth_gateway/token_service.py`

**Fixed Code** (using Redis Lua script for atomicity):
```python
async def refresh_token(self, refresh_token: str) -> Dict[str, str]:
    """Atomic refresh token rotation using Redis Lua script"""
    
    # Lua script for atomic get-delete operation
    lua_script = """
    local key = KEYS[1]
    local value = redis.call('GET', key)
    if value then
        redis.call('DEL', key)
        return value
    else
        return nil
    end
    """
    
    # Atomically get and delete old token
    token_data_str = await self.redis.eval(
        lua_script,
        keys=[f"refresh_token:{refresh_token}"],
        args=[]
    )
    
    if not token_data_str:
        raise HTTPException(status_code=401, detail="Invalid refresh token")
    
    token_data = json.loads(token_data_str)
    self._validate_refresh_token(token_data)
    
    # Generate new tokens
    new_access_token = self._generate_access_token(token_data["user_id"])
    new_refresh_token = self._generate_refresh_token()
    
    # Store new refresh token
    await self.redis.set(
        f"refresh_token:{new_refresh_token}",
        json.dumps({"user_id": token_data["user_id"], ...})
    )
    
    return {
        "access_token": new_access_token,
        "refresh_token": new_refresh_token
    }
```

**Alternative Solution** (using Redis GETDEL command):
```python
# Redis 6.2+ supports GETDEL for atomic get-delete
token_data_str = await self.redis.getdel(f"refresh_token:{refresh_token}")
```

**Solution Characteristics**:
- **Atomic operations**: Lua script executes atomically in Redis
- **No race conditions**: Only one request can successfully GETDEL a token
- **Maintains correctness**: Normal token rotation still works
- **Minimal changes**: Small, focused fix (surgical change)

### Agent Performance

**Run Date**: 2025-11-19  
**Agent**: Claude Code  
**Model**: Claude Sonnet 4.5  
**Environment**: Docker (local)

**Results**:
- **Success**: 1/1 (100%)
- **Reward**: 1.0 (all tests passed)
- **Execution Time**: ~7-8 minutes
  - Environment setup: 3 seconds
  - Agent setup: 16 seconds
  - Agent execution: 4 minutes 8 seconds
  - Verification: 3 minutes 4 seconds
- **Token Usage**:
  - Input tokens: 1,706,720 (99.9% cached)
  - Output tokens: 20,144
  - Estimated cost: ~$8-10 (with caching)

**Agent Trajectory** (simplified):
1. Read `instruction.md` and understand the race condition
2. Explore codebase structure
3. Locate `auth_gateway/token_service.py`
4. Identify non-atomic operations in `refresh_token()`
5. Research Redis atomic operations (Lua scripts, GETDEL)
6. Implement fix using Lua script
7. Test locally (optional, agent-dependent)
8. Verification confirms all tests pass

**Key Success Factors**:
- **Clear problem description**: Agent immediately understood race condition
- **Realistic bug**: Not a trivial typo, required understanding of atomicity
- **Good test coverage**: Tests clearly validated atomicity
- **Appropriate difficulty**: Solvable within timeout, but requires thought

---

## Reusable Components for New Datasets

Harbor-datagen's architecture is designed for extensibility. Most components can be adapted for new evaluation datasets:

### 1. Harbor Task Format (100% Reusable)

The standardized task structure works for any bug-fixing or implementation task:

**Directly Reusable**:
- `task.toml` configuration format
- `instruction.md` natural language description
- `environment/Dockerfile` containerization approach
- `tests/test.sh` + `test_outputs.py` verification pattern
- `solution/solve.sh` reference implementation

**Adaptation Required**:
- Technology stack (Python â†’ JavaScript, Go, Rust, etc.)
- Service types (FastAPI â†’ Express, Django, Rails, etc.)
- Test frameworks (pytest â†’ jest, mocha, cargo test, etc.)

**Example: Adapting to JavaScript/Node.js**:
```
task-name/
â”œâ”€â”€ task.toml                # Same format
â”œâ”€â”€ instruction.md           # Same format
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ Dockerfile          # FROM node:18-alpine
â”‚   â”œâ”€â”€ package.json        # npm dependencies
â”‚   â””â”€â”€ app/                # JavaScript code with bugs
â”œâ”€â”€ solution/
â”‚   â””â”€â”€ solve.sh            # npm install, copy fixes
â””â”€â”€ tests/
    â”œâ”€â”€ test.sh             # npm test
    â””â”€â”€ *.test.js           # Jest tests
```

### 2. Docker Patterns (95% Reusable)

**Multi-Service Environments**:
```dockerfile
FROM python:3.11-slim

# Install multiple services
RUN apt-get update && apt-get install -y \
    redis-server \
    postgresql-client \
    nginx

# Start all services in start.sh
COPY start.sh /workspace/
CMD /workspace/start.sh
```

**Pattern**: `start.sh` orchestrates service initialization with health checks

**Health Check Pattern** (Language-Agnostic):
```bash
# Wait for HTTP service
until curl -s http://localhost:8000/health > /dev/null 2>&1; do
    sleep 1
done

# Wait for Redis
until redis-cli ping > /dev/null 2>&1; do
    sleep 1
done

# Wait for PostgreSQL
until pg_isready -h localhost -p 5432 > /dev/null 2>&1; do
    sleep 1
done
```

### 3. Test Orchestration Pattern (100% Reusable)

**Universal Test Runner** (`tests/test.sh`):
```bash
#!/bin/bash
set -e

# 1. Start services
/workspace/start.sh &
APP_PID=$!

# 2. Create logs directory
mkdir -p /logs/verifier

# 3. Wait for service readiness (with timeout)
timeout=30
counter=0
until [HEALTH_CHECK]; do
    sleep 1
    counter=$((counter + 1))
    if [ $counter -ge $timeout ]; then
        echo "0" > /logs/verifier/reward.txt
        exit 1
    fi
done

# 4. Run language-specific tests
[TEST_COMMAND]  # pytest, npm test, cargo test, go test, etc.

# 5. Write binary reward
if [ $? -eq 0 ]; then
    echo "1" > /logs/verifier/reward.txt
else
    echo "0" > /logs/verifier/reward.txt
fi
```

**Adaptation**: Replace `[HEALTH_CHECK]` and `[TEST_COMMAND]` for your stack

### 4. Agent-Driven Generation Process (90% Reusable)

The 6-phase generation workflow applies to any task type:

**Phase Adaptation**:
- **Phase 1 (Conception)**: Modify prompts for new bug types or domains
- **Phase 2 (Environment)**: Change technology stack, keep structure
- **Phase 3 (Tests)**: Use different test frameworks, keep strategy
- **Phase 4 (Solution)**: Language-specific implementation
- **Phase 5 (Self-Validation)**: Same validation checks
- **Phase 6 (Human Review)**: Domain-specific quality criteria

**Example: Adapting to ML/Data Science Tasks**:
- **Conception**: Data pipeline bugs, model training issues
- **Environment**: Jupyter notebook + pandas + scikit-learn
- **Tests**: Validate data quality, model accuracy, pipeline correctness
- **Solution**: Fixed notebook or Python scripts

### 5. Validation Pipeline (100% Reusable)

The 6-stage validation pipeline is universal:

1. **Build Validation**: Docker build succeeds
2. **Service Validation**: Services start and become ready
3. **Test Discovery**: Tests are discoverable and executable
4. **Correctness Validation**: Tests fail on bugs, pass on fixes
5. **Agent Validation**: Agent can solve within timeout
6. **Quality Review**: Human review for realism

**Checklist** (applies to any task):
```
- [ ] Docker environment builds
- [ ] Services start correctly
- [ ] Tests are discoverable
- [ ] Tests pass/fail correctly
- [ ] Agent can solve end-to-end
- [ ] Problem description is clear
- [ ] Difficulty is appropriate
```

### 6. Harbor Integration (100% Reusable)

Harbor-datagen tasks integrate seamlessly with Harbor's ecosystem:

**Dataset Registration**:
```python
# Add to Harbor registry
{
    "name": "my-custom-dataset",
    "version": "1.0",
    "tasks": [
        {"path": "task1", "difficulty": "easy"},
        {"path": "task2", "difficulty": "medium"},
        {"path": "task3", "difficulty": "hard"}
    ]
}
```

**Evaluation Commands** (unchanged):
```bash
harbor run --dataset my-custom-dataset --agent claude-code
```

### Effort Estimates for New Datasets

| Component | Effort | Reusability |
|-----------|--------|-------------|
| Task format structure | 0 hours | 100% |
| Docker patterns | 0-1 hours | 95% |
| Test orchestration | 1-2 hours | 90% |
| Language-specific tests | 2-4 hours | 0% (new) |
| Agent generation prompts | 2-3 hours | 70% |
| Validation pipeline | 0 hours | 100% |
| Harbor integration | 0 hours | 100% |
| **Total** | **5-10 hours** | **~80%** |

**For New Language/Stack**:
- First task: ~10 hours (setup + first task)
- Subsequent tasks: ~3-5 hours each (reuse setup)

---

## Comparison with Other Frameworks

### SWE-bench

**Architecture**: Repository-level bug fixing with test isolation

**Key Differences**:
- **Scope**: SWE-bench operates on real GitHub repositories (django, sympy, etc.)
- **Bugs**: Real historical bugs from GitHub issues
- **Tests**: Existing repository test suites
- **Isolation**: Complex test environment setup (conda, virtualenv, etc.)
- **Evaluation**: FAIL_TO_PASS + PASS_TO_PASS test verification

**Harbor-datagen vs SWE-bench**:
| Aspect | SWE-bench | Harbor-datagen |
|--------|-----------|----------------|
| Data source | Real GitHub issues | Synthetic (agent-generated) |
| Bug realism | Actual production bugs | Realistic but artificial |
| Test complexity | Large existing test suites | Focused, minimal tests |
| Environment | Repository-specific (complex) | Containerized (standardized) |
| Scalability | Limited by real bugs | Unlimited (agent-generated) |
| Diversity | Limited to open-source projects | Arbitrary tech stacks |

**When to Use Each**:
- **SWE-bench**: Evaluate on real-world, production-quality code
- **Harbor-datagen**: Rapid iteration, controlled difficulty, diverse scenarios

### SWE-Lancer

**Architecture**: Real freelance tasks from Expensify/Upwork with Playwright tests

**Key Differences**:
- **Source**: Real Upwork freelance jobs ($500-$4000 each)
- **Evaluation**: Automated Playwright browser tests
- **Realism**: Actual production application (Expensify App)
- **Scope**: Frontend + backend integration tests
- **Dataset size**: 463 real tasks (limited by real jobs)

**Harbor-datagen vs SWE-Lancer**:
| Aspect | SWE-Lancer | Harbor-datagen |
|--------|------------|----------------|
| Data source | Real Upwork tasks | Synthetic tasks |
| Realism | Actual production app | Realistic scenarios |
| Test type | Playwright (browser) | pytest (backend/API) |
| Task cost | $500-$4000 per task | ~$10 per task (agent generation) |
| Scalability | Limited by real jobs | Unlimited generation |
| Diversity | Single codebase (Expensify) | Any tech stack |

**When to Use Each**:
- **SWE-Lancer**: Evaluate on real-world freelance-quality tasks
- **Harbor-datagen**: Rapid dataset growth, diverse technology stacks

### Terminal-Bench

**Architecture**: Benchmark aggregator for multiple evaluation datasets

**Key Differences**:
- **Purpose**: Unified interface for running multiple benchmarks
- **Datasets**: Integrates SWE-bench, DevEval, LiveCodeBench, etc.
- **Format**: Converts external benchmarks to Harbor format
- **Evaluation**: Consistent evaluation across diverse benchmarks

**Harbor-datagen vs Terminal-Bench**:
| Aspect | Terminal-Bench | Harbor-datagen |
|--------|----------------|----------------|
| Purpose | Benchmark aggregation | Data generation |
| Data source | Existing benchmarks | Agent-generated |
| Format | Converts to Harbor | Native Harbor |
| Focus | Breadth (many benchmarks) | Depth (quality tasks) |

**Relationship**: Harbor-datagen tasks can be added to Terminal-Bench as a dataset

### Architecture Comparison Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EVALUATION FRAMEWORK LANDSCAPE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SWE-bench
â”œâ”€ Real GitHub repositories (django, sympy, requests, etc.)
â”œâ”€ Historical production bugs
â”œâ”€ Complex test environments
â””â”€ Strength: Real-world realism

SWE-Lancer  
â”œâ”€ Real Upwork freelance tasks (Expensify App)
â”œâ”€ Playwright browser tests
â”œâ”€ Production application
â””â”€ Strength: Real freelance job quality

Harbor-datagen (This Project)
â”œâ”€ Agent-generated synthetic tasks
â”œâ”€ Focused, containerized environments
â”œâ”€ Controlled difficulty and diversity
â””â”€ Strength: Scalable, rapid iteration

Terminal-Bench
â”œâ”€ Aggregates all of the above
â”œâ”€ Unified Harbor format
â”œâ”€ Consistent evaluation interface
â””â”€ Strength: Comprehensive coverage
```

---

## Best Practices & Guidelines

### Task Design Principles

#### 1. Realism Over Artificiality

**Good** (auth_token_race_condition):
```
"Users are getting randomly logged out under load"
```

**Bad**:
```
"There is a race condition in the token refresh endpoint. Fix it using 
atomic Redis operations."
```

**Principle**: Describe symptoms from a user's perspective, not the technical solution.

#### 2. Subtle Bugs, Not Obvious Typos

**Good**: Non-atomic Redis operations (requires understanding of concurrency)

**Bad**: Missing semicolon, typo in variable name

**Principle**: Bugs should require analysis and understanding, not just syntax fixing.

#### 3. Comprehensive Test Coverage

**Test Strategy**:
- **Success cases**: Normal operation works
- **Edge cases**: Boundary conditions handled
- **Concurrency**: Stress concurrent operations
- **Failure modes**: Errors handled gracefully

**Example Test Weights** (auth_token_race_condition):
- 40%: Concurrent refresh (validates atomicity)
- 25%: Sequential refresh (validates correctness)
- 35%: Multi-user stress (validates scale)

#### 4. Appropriate Difficulty Calibration

**Difficulty Guidelines**:
- **Easy**: Agent success rate 70-90%
  - Simple bugs (logic errors, missing validation)
  - Clear problem descriptions
  - Minimal codebase exploration
  
- **Medium**: Agent success rate 40-70%
  - Moderate bugs (async issues, state management)
  - Requires code exploration
  - Multiple potential solutions
  
- **Hard**: Agent success rate 20-40%
  - Complex bugs (race conditions, distributed systems)
  - Requires deep understanding
  - Non-obvious solutions

**Calibration Process**:
1. Agent attempts task during self-validation
2. Measure success rate, execution time, token usage
3. Adjust difficulty tag if mismatch
4. Iterate until calibrated

### Resource Configuration

#### Timeout Guidelines

**Agent Timeout** (`agent.timeout_sec`):
- **Default**: 600 seconds (10 minutes)
- **Complex debugging**: 900 seconds (15 minutes)
- **Simple tasks**: 300 seconds (5 minutes)

**Verifier Timeout** (`verifier.timeout_sec`):
- **Default**: 180 seconds (3 minutes)
- **Large test suites**: 300 seconds (5 minutes)
- **Integration tests**: 240 seconds (4 minutes)

**Build Timeout** (`environment.build_timeout_sec`):
- **Default**: 600 seconds (10 minutes)
- **Heavy dependencies**: 900 seconds (15 minutes)
- **Minimal images**: 300 seconds (5 minutes)

#### Memory and CPU Guidelines

**Standard Configuration** (most tasks):
```toml
[environment]
cpus = 2
memory_mb = 4096
storage_mb = 20480
```

**Database-Heavy Tasks**:
```toml
[environment]
cpus = 4
memory_mb = 8192  # PostgreSQL benefits from more memory
storage_mb = 40960
```

**CPU-Intensive Tasks** (data processing, ML):
```toml
[environment]
cpus = 8
memory_mb = 16384
storage_mb = 51200
```

### Docker Optimization

#### Image Size Optimization

**Use Slim Base Images**:
```dockerfile
# Good: Slim image (150MB)
FROM python:3.11-slim

# Bad: Full image (1GB)
FROM python:3.11
```

**Multi-Stage Builds** (if applicable):
```dockerfile
# Build stage
FROM node:18 AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

# Runtime stage
FROM node:18-alpine
COPY --from=builder /app/dist /app/dist
CMD ["node", "/app/dist/server.js"]
```

**Clean Up APT Cache**:
```dockerfile
RUN apt-get update && apt-get install -y redis-server \
    && rm -rf /var/lib/apt/lists/*
```

#### Build Performance

**Pin Dependency Versions**:
```txt
fastapi==0.109.0
redis==5.0.1
pydantic==2.5.3
```

**Use Docker Build Cache**:
```dockerfile
# Copy requirements first (cached if unchanged)
COPY requirements.txt /workspace/
RUN pip install -r requirements.txt

# Copy code last (changes frequently)
COPY app/ /workspace/app/
```

### Test Design Best Practices

#### 1. Test Independence

Each test should be independent:
```python
# Good: Create fresh user per test
@pytest.mark.asyncio
async def test_concurrent_refresh(http_client):
    user = await create_test_user(http_client, "test_user_1")
    # ... test logic

# Bad: Rely on global state
user = None  # Module-level variable

@pytest.mark.asyncio
async def test_concurrent_refresh(http_client):
    global user
    # ... test logic (depends on previous tests)
```

#### 2. Deterministic Tests

Avoid timing-based assertions:
```python
# Good: Poll with timeout
async def wait_for_job_completion(job_id, timeout=30):
    start = time.time()
    while time.time() - start < timeout:
        status = await get_job_status(job_id)
        if status == "completed":
            return True
        await asyncio.sleep(0.5)
    return False

# Bad: Fixed sleep
await asyncio.sleep(5)  # Hope job completes in 5 seconds
assert job.status == "completed"
```

#### 3. Clear Failure Messages

Provide context in assertions:
```python
# Good: Descriptive assertion
assert success_count == 1, f"Expected exactly 1 success, got {success_count}"

# Bad: No context
assert success_count == 1
```

#### 4. Cleanup Resources

Clean up after tests:
```python
@pytest_asyncio.fixture(scope="function")
async def redis_client():
    client = redis.Redis(host="localhost", decode_responses=True)
    yield client
    # Cleanup: flush test data
    await client.flushdb()
    await client.close()
```

### Agent Generation Guidelines

#### Prompting Strategies

**Phase 1: Task Conception**:
```
Generate a realistic backend bug scenario:
- Difficulty: Hard
- Category: Backend engineering
- Technology: Python + FastAPI + Redis
- Bug type: Race condition, concurrency issue, or database problem
- Constraint: Solvable in 10 minutes by an expert engineer

Describe the bug from a user's perspective (symptoms, not causes).
```

**Phase 2: Environment Creation**:
```
Create a complete Docker environment for the task:
- Use python:3.11-slim base image
- Install necessary services (Redis, PostgreSQL, etc.)
- Write buggy application code (subtle bug, not typo)
- Create startup scripts with health checks
- Pin all dependency versions

The environment must build successfully and services must start reliably.
```

**Phase 3: Test Development**:
```
Design a comprehensive test suite:
- Test 1: Validates the core bug (e.g., concurrent requests)
- Test 2: Validates correct behavior (e.g., sequential requests)
- Test 3: Stress test under load

Use pytest with async support. Tests must:
- Fail on buggy code
- Pass on fixed code
- Complete within 180 seconds
- Be deterministic (no flaky tests)
```

#### Validation Prompts

**Self-Validation Phase**:
```
Attempt to solve the task you generated:
1. Read instruction.md
2. Explore the codebase
3. Identify the bug
4. Implement a fix
5. Verify tests pass

If you cannot solve it within 10 minutes, simplify the task.
```

### Common Pitfalls and Solutions

#### Pitfall 1: Flaky Tests

**Problem**: Tests pass/fail non-deterministically

**Solution**:
- Use health checks instead of fixed sleeps
- Implement retry logic with timeouts
- Avoid relying on exact timing

```python
# Good
async def wait_for_condition(check_fn, timeout=30):
    start = time.time()
    while time.time() - start < timeout:
        if await check_fn():
            return True
        await asyncio.sleep(0.1)
    return False

# Bad
await asyncio.sleep(5)  # Hope condition is met
```

#### Pitfall 2: Services Fail to Start

**Problem**: Tests timeout because services aren't ready

**Solution**:
- Implement robust health checks
- Increase startup timeout
- Log service startup progress

```bash
# Good: Health check with logging
echo "Waiting for Redis..."
timeout=30
counter=0
until redis-cli ping > /dev/null 2>&1; do
    sleep 1
    counter=$((counter + 1))
    echo "Still waiting... ($counter/$timeout)"
    if [ $counter -ge $timeout ]; then
        echo "FAILED: Redis did not start"
        exit 1
    fi
done
echo "Redis is ready"
```

#### Pitfall 3: Trivial Bugs

**Problem**: Agent solves task too easily (100% success on first try)

**Solution**:
- Make bug more subtle (not a typo)
- Increase code complexity
- Add distractors (multiple potential bug locations)
- Require understanding of concepts (atomicity, async, transactions)

#### Pitfall 4: Ambiguous Problem Descriptions

**Problem**: Agent doesn't understand the problem

**Solution**:
- Add more context (when does the bug occur?)
- Include user symptoms (what do users experience?)
- Provide hints (not solutions) if needed
- Test problem description during self-validation

---

## Conclusion

Harbor-datagen demonstrates a scalable approach to evaluation dataset creation through agent-driven generation. By combining:

1. **Standardized task format** (Harbor-compatible)
2. **Agent-driven generation** (6-phase workflow)
3. **Robust validation** (6-stage pipeline)
4. **Seamless integration** (Harbor ecosystem)

...the system enables rapid iteration on high-quality evaluation tasks at ~$10 per task (vs. $500-4000 for real-world tasks).

**Current Status**: 3 validated tasks (2 production-ready) with 91-100% agent success rates

**Future Directions**:
- Scale to 50+ production tasks
- Expand to new languages (JavaScript, Go, Rust)
- Integrate multi-file code changes
- Support frontend + backend integration tasks
- Add reinforcement learning task generation

**For Contributors**: See `AGENTS.md` for detailed guidance on creating new tasks with AI coding agents.

**For Researchers**: See example runs in `example_runs/` directories for agent trajectory data useful for training and analysis.

**For Engineers**: Use Harbor's evaluation framework to benchmark your coding agents on these realistic, validated tasks.

---

**References**:
- Harbor Framework: https://harborframework.com
- Terminal-Bench: https://tbench.ai
- SWE-bench: https://www.swebench.com
- SWE-Lancer: https://arxiv.org/abs/2502.12115
- AGENTS.md specification: https://agents.md/

